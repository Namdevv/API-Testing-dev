services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    volumes:
      - ./models:/root/.cache/huggingface/hub/
    ports:
      - "8000:8000"
    # command: >
    #   --model unsloth/Qwen2.5-3B-Instruct-bnb-4bit
    #   --dtype bfloat16
    #   --max-model-len 8192
    #   --gpu-memory-utilization 0.6
    #   --quantization bitsandbytes
    #   --load-format bitsandbytes
    #   --enable-lora
    #   --max-lora-rank 64
    #   --lora-modules reasoning-bot=Namtran0912/Qwen2.5-3B-Instruct-lora-adapter-v3
    command: >
      --model Qwen/Qwen2.5-3B-Instruct-AWQ
      --quantization awq_marlin
      --dtype bfloat16
      --max-model-len 8192
      --gpu-memory-utilization 0.6
      --enable-lora
      --max-lora-rank 64
      --lora-modules reasoning-bot=Namtran0912/Qwen2.5-3B-Instruct-lora-adapter-v3
      